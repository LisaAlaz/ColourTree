# -*- coding: utf-8 -*-
"""ColourTree_lib.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FGBUVv2gpVKJ6wuudjpW4UK-qLciX7KM
"""

import math
import copy
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
from matplotlib import colors as mcolors

"""## Functions to build the tree"""

def decision_tree(data, depth):
    #Takes a dataset and a depth value as inputs (depth value initialised to 0 to
    #start building the tree from the root) and returns a binary tree stored as a
    #dictionary of dicionaries and its total height. It is recursively applied to
    #each new subset which thus becomes the root of a new subtree of the tree.
    if np.all(data[1:,-1] == data[0,-1]):
        return (data[0,-1], depth) 
    else:
        attribute, value, l_dataset, r_dataset = find_split(data)
        majority_cl = majority_class(data)
        l_branch, l_depth = decision_tree(l_dataset, depth + 1) 
        r_branch, r_depth = decision_tree(r_dataset, depth + 1) 
        node = {'attribute':attribute, 'checked_prune': False, 'value':value, 'depth': depth, 
                'majority_class': majority_cl, 'left':l_branch, 'right':r_branch}        
        return (node, max(l_depth, r_depth))

def majority_class(data):
    #Returns the majority class label in a dataset.
    rooms = list(data[:,-1])
    room_count = dict.fromkeys(set(rooms), 0)
    for room_number in room_count.keys():
        room_count[room_number] = rooms.count(room_number)
    max_room = 0
    max_value = 0   
    for room, count in room_count.items():
        if count > max_value:
            max_value = count
            max_room = room
    return max_room

def find_split(data):  
    #Finds the best possible split of a multi-dimensional dataset, given as input. 
    #It tests multiple split points within and across dimensions, and chooses the
    #split that gives the greatest informations gain. Returns the dimension on which 
    #the dataset was split, the split point value and the two resulting sub-datasets.
    best_splits = [] 
    best_attribute = 0
    best_gain = 0 
    best_splitvalue = 0
    best_l_dataset = 0
    best_r_dataset = 0
    for attribute in range(data.shape[1] - 1): 
        sorted_data = data[np.argsort(data[:, attribute])] 
        for i, j in zip(np.unique(sorted_data[:,attribute]), 
                        np.unique(sorted_data[1:,attribute])): 
            splitvalue = (i + j)/2 
            l_dataset = split(sorted_data, sorted_data[:,attribute]<splitvalue)[0] 
            r_dataset = split(sorted_data, sorted_data[:,attribute]<splitvalue)[1]
            gain = compute_gain(data, l_dataset, r_dataset)
            if gain > best_gain:
                best_gain = gain
                best_splitvalue = splitvalue 
                best_l_dataset = l_dataset
                best_r_dataset = r_dataset
                best_attribute = attribute
    return (best_attribute, best_splitvalue, best_l_dataset, best_r_dataset)

def split(data, value):
    #Takes an array and a value as input, and returns two arrays split on that value.
    return [data[value], data[~value]]

def compute_gain(data, l_data, r_data):
    #Computes and returns the information gain of a certain dataset split given
    #the parent dataset and the two children subsets resulting from the split.
    total_size = data.shape[0] 
    l_size = l_data.shape[0] 
    r_size = r_data.shape[0] 
    gain = compute_entropy(data) - (((l_size / total_size) * compute_entropy(l_data)) 
                                    + ((r_size / total_size) * compute_entropy(r_data)))
    return gain

def compute_entropy(data):
    #Takes an array dataset as input, and outputs its information entropy.  
    samples = data.shape[0] 
    labels, counts = np.unique(data[:,-1], return_counts=True) 
    entropy = 0 
    for i in range(len(counts)):
        if counts[i] > 0: 
            entropy += np.negative((counts[i] / samples) * math.log2(counts[i] / samples)) 
    return entropy

"""## Functions to classify the data"""

def predict_class_of_sample(my_tree, data_line):
    #Takes a one-dimensional array (i.e. one sample) and classifies it using the 
    #decision tree. It returns an integer (the predicted room number).
    if type(my_tree) == dict:
        if data_line[my_tree['attribute']] < my_tree['value']:     
            if type(my_tree['left'])==dict:
                my_new_tree = my_tree['left']
                return predict_class_of_sample(my_new_tree, data_line)
            else:
                return my_tree['left']
        else:
            if type(my_tree['right'])==dict:
                my_new_tree = my_tree['right']
                return predict_class_of_sample(my_new_tree, data_line)
            else:
                return my_tree['right']
    else:
        return my_tree

def predict_dataset_classes(data, my_tree):
    #Predicts the classes of the data samples of a full data set. Returns a list 
    #of integers corresponding to the predicted room number of each sample.
    predicted_class_lst = []
    n_samples = data.shape[0]
    for i in range(n_samples):
        predicted_class_lst.append(predict_class_of_sample(my_tree, data[i]))
    return predicted_class_lst

"""## Functions for evaluation"""

def count_classes(i, j, data, pred_class_list):
    #Counts the pairs i, j of actual and predicted classes (respectively) 
    #in a data set. Returns the count as an integer.
    count = 0 
    for k in range(data.shape[0]):
        if data[k][-1] == i and pred_class_list[k] == j:
            count = count +1
    return count

def confusion_matrix(data, pred_list, n_classes): 
    #Computes the confusion matrix.
    conf_matrix = np.zeros((n_classes,n_classes))
    for i in range(n_classes):
        for j in range(n_classes):
            conf_matrix[i][j]=count_classes(i+1,j+1,data,pred_list)
    return conf_matrix

def norm_confusion_matrix(confusion_matrix):
    #Computes the normalised confusion matrix.
    norm_matrix = np.zeros_like(confusion_matrix, dtype = float)
    for i in range(confusion_matrix.shape[0]):
        for j in range(confusion_matrix.shape[1]):
            if sum(confusion_matrix[i]) != 0:
                norm_matrix[i][j] = confusion_matrix[i][j]/sum(confusion_matrix[i])                
    return norm_matrix

def accuracy(conf_matrix, room_n):
    #Computes the accuracy for a selected class and returns it as a float.
    tp = conf_matrix[room_n-1][room_n-1]
    tn = 0
    for i in [number for number in range(4) if number != room_n-1]:
        for j in [number for number in range(4) if number != room_n-1]:
            tn = tn + conf_matrix[i][j]
    total_samples = np.sum(conf_matrix)
    return ((tp + tn)/total_samples)

def overall_accuracy(conf_matrix):  
    #Computes the overall accuracy over all classes and returns it as a float.
    #A note of caution: it works well for balanced data sets but can be misleading 
    #for imbalanced ones.
    accuracy_list =[]
    for i in range(1,5):
        accuracy_list.append(accuracy(conf_matrix, i))
    return sum(accuracy_list)/len(accuracy_list)

def precision(conf_matrix, room_n):
    #Computes the precision for a selected class and returns it as a float.
    tp = conf_matrix[room_n-1][room_n-1]
    fp = 0
    for i in [number for number in range(4) if number != room_n-1]:
        fp = fp + conf_matrix[room_n-1][i]
    return tp/(tp+fp)

def recall(conf_matrix, room_n):
    #Computes the recall for a selected class and returns it as a float.
    tp = conf_matrix[room_n-1][room_n-1]
    fn = 0
    for j in [number for number in range(4) if number != room_n-1]:
        fn = fn + conf_matrix[j][room_n-1]
    return tp/(tp+fn)

def f_measure(conf_matrix, room_n, beta):
    #Computes the F-score for a selected class and returns it as a float. This is 
    #the general version of the F-score formula, to calculate F1 just input weight
    #parameter beta = 1.
    return (1+(beta**2))*(precision(conf_matrix, room_n)*recall(conf_matrix, room_n))/(((beta**2)*(precision(conf_matrix, room_n))+recall(conf_matrix, room_n)))

def max_depth(tree, depth = 0):
    #Returns the maximum depth of a tree.
    if type(tree['left']) != dict and type(tree['right']) != dict:
        depth = depth+1
        return depth
    elif type(tree['left']) == dict and type(tree['right']) != dict:
        depth = tree['left']['depth']
        tree = tree['left']
        return max_depth(tree, depth)
    elif type(tree['right']) == dict and type(tree['left']) != dict:
        depth = tree['right']['depth']
        tree = tree['right']
        return max_depth(tree, depth)  
    else:
        depth = depth + 1
        l_depth = max_depth(tree['left'], depth)
        r_depth = max_depth(tree['right'], depth)
        if l_depth > r_depth:
            return l_depth
        else:
            return r_depth

def k_fold_evaluation(list_of_k_fold):
    #Evaluation function for cross-validation.
    unpruned_accuracy = np.zeros(10) 
    pruned_accuracy = np.zeros(10) 
    unpruned_accuracy_val = np.zeros(10) 
    pruned_accuracy_val = np.zeros(10) 
    
    unpruned_conf_matrix_norm_total = np.zeros((4,4)) 
    pruned_conf_matrix_norm_total = np.zeros((4,4))
    unpruned_conf_matrix_norm_total_val = np.zeros((4,4)) 
    pruned_conf_matrix_norm_total_val = np.zeros((4,4))

    for test_number in range(10):
        test_set,training_set_total = cross_validation_sets(test_number, list_of_k_fold)
        list_of_val_train = k_fold(training_set_total, 9)
        val_data, training_set = cross_validation_sets(0,list_of_val_train)
        tree_trained = decision_tree(training_set, depth=0)
        prune_tree, original_tree, iteration = pruning_iteration(tree_trained[0], val_data)

        unpruned_pred_class = predict_dataset_classes(test_set,original_tree)
        pruned_pred_class = predict_dataset_classes(test_set,prune_tree)
        unpruned_pred_class_val = predict_dataset_classes(val_data,original_tree)
        pruned_pred_class_val = predict_dataset_classes(val_data,prune_tree)

        norm_conf_matrix_unpruned = norm_confusion_matrix(confusion_matrix(test_set, unpruned_pred_class, 4))
        norm_conf_matrix_pruned = norm_confusion_matrix(confusion_matrix(test_set, pruned_pred_class, 4))
        norm_conf_matrix_unpruned_val = norm_confusion_matrix(confusion_matrix(val_data, unpruned_pred_class_val, 4))
        norm_conf_matrix_pruned_val = norm_confusion_matrix(confusion_matrix(val_data, pruned_pred_class_val, 4))

        unpruned_conf_matrix_norm_total += norm_conf_matrix_unpruned
        pruned_conf_matrix_norm_total += norm_conf_matrix_pruned
        unpruned_conf_matrix_norm_total_val += norm_conf_matrix_unpruned_val
        pruned_conf_matrix_norm_total_val += norm_conf_matrix_pruned_val
    
        unpruned_accuracy[test_number] = overall_accuracy(norm_conf_matrix_unpruned)
        pruned_accuracy[test_number] = overall_accuracy(norm_conf_matrix_pruned)
        unpruned_accuracy_val[test_number] = overall_accuracy(norm_conf_matrix_unpruned_val)
        pruned_accuracy_val[test_number] = overall_accuracy(norm_conf_matrix_pruned_val)

    unpruneed_final_conf_matrix_norm = unpruned_conf_matrix_norm_total*10
    pruneed_final_conf_matrix_norm = pruned_conf_matrix_norm_total*10
    unpruneed_final_conf_matrix_norm_val = unpruned_conf_matrix_norm_total_val*10
    pruneed_final_conf_matrix_norm_val = pruned_conf_matrix_norm_total_val*10

    total_unpruned_acc=np.average(unpruned_accuracy)*100
    total_pruned_acc=np.average(pruned_accuracy)*100
    total_unpruned_acc_val=np.average(unpruned_accuracy_val)*100
    total_pruned_acc_val=np.average(pruned_accuracy_val)*100

    return unpruneed_final_conf_matrix_norm, pruneed_final_conf_matrix_norm, total_unpruned_acc, total_pruned_acc, unpruneed_final_conf_matrix_norm_val, pruneed_final_conf_matrix_norm_val, total_unpruned_acc_val, total_pruned_acc_val

"""## Functions for data preparation"""

def shuffle_data(data):
    #Shuffles the rows in a dataset.
    np.random.shuffle(data)
    return data

def k_fold(data, k):
    #Splits a dataset into k sub-datasets. If the total number of samples is not 
    #divisible by k then it splits into (k-1) subsets of fixed size and puts the 
    #remaining samples into the last subset.
    number_of_data_samples = data.shape[0]
    fold_size = round(number_of_data_samples / k)
    remainder = number_of_data_samples % k
    list_of_folds = []
    start_idx = 0
    end_idx = 0
    while start_idx < number_of_data_samples and end_idx < number_of_data_samples:
        if len(list_of_folds)==(k-1) and (remainder !=0):
            fold = data[start_idx:, :]
            list_of_folds.append(fold)
            return list_of_folds
        else:
            end_idx = end_idx + fold_size
            fold = data[start_idx:end_idx, :]
            list_of_folds.append(fold)
            start_idx = end_idx  
    return list_of_folds

def cross_validation_sets(test_number, list_of_folds):
    #Takes a list of folds generated by the k_fold function. Designates one fold
    #as the test set and all the others collectively as the training set.
    test_set = list_of_folds[test_number]
    list_of_other_indices = [i for i in range(len(list_of_folds)) if i!=test_number]
    training_set = list_of_folds[list_of_other_indices[0]]
    for idx in list_of_other_indices[1:]:
        training_set= np.concatenate((training_set, list_of_folds[idx]), axis=0)
    return test_set,training_set

"""## Functions for plotting the tree"""

def tree_nodes(tree, width_distance, depth_distance):
    #Converts a tree stored as a dictionary of dictionaries to a representation 
    #of its edges and their location on the x-y plane.
    all_nodes = []

    def get_nodes(node, offset, width_distance, depth_distance):
        #Appends each node to the global list of nodes with coordinates. 
        #Recursive function: starts a node and gets its left and right children.
        if(type({}) == type(node)):
            left_node = {}
            left_node["startpoint"] = {'data':(node['attribute'], node['value']), 
                                       'y':offset, 'x':node['depth'] * depth_distance}
            right_node = {}
            right_node["startpoint"] = {'data':(node['attribute'], node['value']), 
                                        'y':offset, 'x':node['depth'] * depth_distance}
            left_node["endpoint"] = {'data':get_nodes(node['left'], 
                                     offset - width_distance/2, width_distance/2, 
                                     depth_distance), 'y':offset - width_distance/2, 
                                     'x':depth_distance * (node['depth'] + 1)}
            right_node["endpoint"] = {'data':get_nodes(node['right'], 
                                      offset + width_distance/2, width_distance/2, 
                                      depth_distance), 'y':offset + width_distance/2, 
                                      'x':depth_distance * (node['depth'] + 1)}
            all_nodes.append(left_node)
            all_nodes.append(right_node)
            return (node['attribute'], node['value'])
        else:
            return node

    #all appended nodes are returned by the outer function
    get_nodes(tree, 0, width_distance, depth_distance)
    return all_nodes

def tree_edges(all_nodes):
    #Returns a litst of coordinates for the start and end points of the edges of 
    #the tree, to be plotted on the plane.
    edges = []
    for n in all_nodes:
        startX = n['startpoint']['x']
        startY = n['startpoint']['y']
        endX = n['endpoint']['x']
        endY = n['endpoint']['y'] 
        #the line is made of two points - start and end
        edges.append([[startX, startY], [endX, endY]])
    return edges

def draw_tree(tree):
    #Plots the tree on the x-y plane. Takes input in the same form output by the 
    #decision_tree() function, that is, a tuple with a tree expressed 
    #as nested dictionaries as first element and its total height as second element.
    width_distance = 1500
    depth_distance = 1000
    figsizeX = 25
    figsizeY = 35
    starting_node = tree[0]
    levels = tree[1]
    all_nodes = tree_nodes(starting_node, width_distance, depth_distance)    
    lines = tree_edges(all_nodes)    
    colors = [mcolors.to_rgba(c)
              for c in plt.rcParams['axes.prop_cycle'].by_key()['color']]
    line_segments = LineCollection(lines, linewidths=1, colors=colors, linestyle='solid')
    fig, ax = plt.subplots(figsize=(figsizeX, figsizeY))
    ax.set_xlim(-1, (levels + 1) * (depth_distance + 1))
    ax.set_ylim(-1 * width_distance, 1 * width_distance)
    ax.add_collection(line_segments)

    for n in all_nodes:      
        if(0 == n['startpoint']['x']):
            startX = n['startpoint']['x']
            startY = n['startpoint']['y']
            xy = (startX, startY)
            ax.annotate('(%s, %s)' % n['startpoint']['data'], xy=xy, textcoords='data')            
        endX = n['endpoint']['x']
        endY = n['endpoint']['y']         
        xy = (endX, endY)
        if(type(()) == type(n['endpoint']['data'])):
            ax.annotate('(%s, %s)' % n['endpoint']['data'], xy=xy, textcoords='data')
        else:
            ax.annotate('(%s)' % n['endpoint']['data'], xy=xy, textcoords='data')

    plt.show()

"""## Functions for pruning"""

def replace_node(tree_node, validation_set, child):
    #Takes as input a tree_node eligible for the pruning of its left and right children.
    #Checks the accuracy (average accuracy for class 1-4) of the pruned node vs the unpruned 
    #node on the data subset. If accuracy is improved we replace the node with the majority class.
    #We reset the 'check_pruned' flag in the original tree to False if we make changes, as the 
    #parent node could be eligible for further pruning.
    change =0
    if validation_set.shape[0]==0:
        return tree_node, change
    unpruned_pred = predict_dataset_classes(validation_set, tree_node[str(child)])
    unpruned_conf_mat = norm_confusion_matrix(confusion_matrix(validation_set, unpruned_pred, 4))
    accuracy_unpruned = overall_accuracy(unpruned_conf_mat)
    
    pruned_node = tree_node[str(child)]['majority_class']
    pruned_pred = predict_dataset_classes(validation_set, pruned_node)
    pruned_conf_mat = norm_confusion_matrix(confusion_matrix(validation_set, pruned_pred, 4))
    accuracy_pruned = overall_accuracy(pruned_conf_mat)
    
    if accuracy_pruned >= accuracy_unpruned:
        change = 1
        tree_node[str(child)]=tree_node[str(child)]['majority_class']
        tree_node['checked_prune']= False
        return tree_node, change
    else:
        change = 0
        tree_node['checked_prune']= True
        return tree_node, change

def prune(tree_node, validation_data):
    #Implements the pruning of pre-terminal nodes.
    if type(tree_node)!=dict or validation_data.shape[0]==0 or tree_node['checked_prune']==True:
        return      
    else:
        validation_data_left = validation_data[validation_data[:, int(tree_node['attribute'])] <= tree_node['value']]
        validation_data_right = validation_data[validation_data[:, int(tree_node['attribute'])] > tree_node['value']]
        if type(tree_node['left'])==dict and type(tree_node['right'])!=dict:
            if type(tree_node['left']['left']) != dict and type(tree_node['left']['right']) != dict:              
                tree_node['checked_prune'] = True
                tree_node, change = replace_node(tree_node, validation_data_left, "left")   
            else:
                prune(tree_node['left'], validation_data_left)
        elif type(tree_node['right'])==dict and type(tree_node['left'])!=dict:
            if type(tree_node['right']['left']) != dict and type(tree_node['right']['right']) != dict:
                tree_node['checked_prune'] = True
                tree_node, change = replace_node(tree_node, validation_data_right, "right")
            else:
                prune(tree_node['right'], validation_data_right)
        else: 
            prune(tree_node['left'], validation_data_left)
            prune(tree_node['right'], validation_data_right)

def pruning_iteration(original_tree, validation_data):
    #Makes a deep copy of the original tree and iteratively prunes checking whether accuracy improves.
    whole_tree_to_prune = copy.deepcopy(original_tree)
    list_of_accuracies = []
    
    pruned_pred = predict_dataset_classes(validation_data, original_tree)
    pruned_conf_mat = norm_confusion_matrix(confusion_matrix(validation_data, pruned_pred, 4))
    accuracy_unpruned = overall_accuracy(pruned_conf_mat)
    
    list_of_accuracies.append(accuracy_unpruned)
    finished = False
    iteration = 0
    while not finished:
        iteration +=1
        prune(whole_tree_to_prune, validation_data)
        pruned_pred = predict_dataset_classes(validation_data,whole_tree_to_prune )
        pruned_conf_mat = norm_confusion_matrix(confusion_matrix(validation_data, pruned_pred, 4))
        accuracy_pruned = overall_accuracy(pruned_conf_mat)
        list_of_accuracies.append(accuracy_pruned)

        if list_of_accuracies[iteration] > list_of_accuracies[iteration-1]:
            print("Accuracy improving")
        else:
            finished = True
    
    return whole_tree_to_prune, original_tree, iteration